% !TEX root =  main.tex

\section{Analysis of Algorithm}

As explained in \Cref{sec:algorithm}, our algorithm can be viewed as a \emph{sampling procedure}, where we repeatedly sample a nonzero entry of the tensor and evaluate $x_g$ on the selected entry, for all parameter settings~$g$ simultaneously.
Lastly, our algorithm averages these values for each $g$, to obtain an estimate of $f_g(\mathcal{A})$ for all $g$.
Suppose that our procedures samples a total of $S$ nonzeros of $\mathbf{A}$, which are located at positions $I_1, I_2, \dots, I_S$.
For efficiency reasons, we want to select $S$ as small as possible, while still having provable guarantees on the concentration of our unbiased estimator $\sum_i x_g(I_i) / S$.
We give two concentration bounds for our estimator: one that assumes that the samples $I_j$ are sampled \emph{with replacement}, using Hoeffding's inequaliy, and an improved version that assumes that the samples $I_j$ are sampled without replacement, using the recent Hoeffding-Serfling inequality due to Bardenet and Maillard \cite{BM2015}.
Although the two concentration bounds are identical as the number of nonzeroes $k(\mathcal{A})$ grows, the two bounds differ when the number of nonzeroes is small.

\subsection{A concentration bound when sampling with replacement}

We make use of Hoeffding's inequality, which we state here for completeness:

\begin{thm}[\cite{Hoeffding}] \label{thm:hoeffding}
Let $X_1, X_2, \dots, X_n$ be be $n$ independent random variables bounded such that $0 \leq X_i \leq 1$.
Let $\bar{X} = \frac{1}{n} \sum_{i = 1}^n X_i$ be their mean.
Then for any $t \geq 0$,
\[
\Pr[|X - \E[X]| \geq t] \leq 2\exp(-2nt^2)
\]
\end{thm}fra

Observe that for any parameter setting~$g$ and any tensor element $\mathbf{i}$, the value $x_g(\mathbf{i})$ is a random variable bounded between 0 and 1.
Furthermore, since the entries $I_1, I_2, \dots, I_S$ are sampled independently from among the nonzeroes, the random variables $x_g(\mathbf(I_1)), x_g(\mathbf(I_2)), \dots, x_g(\mathbf{I_S})$ are independent.
We can therefore apply \Cref{thm:hoeffding} to obtain our first concentration bounds:

\begin{thm} \label{thm:with-replacement}
If $S \geq \frac{N B^N}{2 \eps^2} \log \left( \frac{2B}{\delta} \right)$, then
\[
\Pr\left[\bigcup_g (1 - \eps) f_g \leq F_g \leq (1 + \eps) f_g\right] \geq 1 - \delta
\]
\end{thm}

\begin{proof}
By definition, $F_g = \frac{1}{S} \sum_{i = 1}^S x_g(\mathbf{I}_i)$
As discussed in \todo{Need citation here}, $\E[F_g] = f_g$ and $x_g(\mathbf{I}_1, \mathbf{I}_2, \dots, \mathbf{I}_S$ are independent and bounded between 0 and 1.
By \Cref{thm:hoeffding}, we have
\[
\Pr[|F_g - f_g| \geq \eps f_g]
\leq 2\exp(-2 S \eps^2 f_g^2)
\leq 2\exp(-2 S \eps^2 / B^N)
\]
since $f_g$ is an average of $S$ values, each of which is at least $1/B^N$, since each block has size at most $B^N$ entries and thus $B^N$ nonzero entries.
By the union bond over the $B^N$ possible parameter settings $g$, we obtain that
\[
\Pr\left[ \bigcup_g |F_g - f_g| \geq \eps f_g \right]
\leq 2 B^N \exp(-2 S \eps^2 / B^N)
\]
Substituting $S \geq \frac{N B^N}{2 \eps^2} \log \left( \frac{2B}{\delta} \right)$, we obtain that
\[
\Pr\left[ \bigcup_g |F_g - f_g| \geq \eps f_g \right] \leq \delta
\]
and so since $1/(1 + \eps) \geq 1 - \eps$ by the Taylor expansion of $1/(1 + \eps)$ about 0, we obtain
\[
\Pr\left[ \bigcup_g (1 - \eps) f_g \leq F_g \leq (1 + \eps) f_g\right] \geq 1 - \Pr\left[\bigcup_g |F_g - f_g| \geq \eps f_g \right] \geq 1 - \delta \qedhere
\]
\end{proof}

Note that this bound is independent of $k(\mathcal{A})$ in a direct manner.
Clearly, obtaining a high probability bound with $\delta \leq 1/k(\mathcal{A})^c$ for some $c$ would indeed require dependence on $k(\mathcal{A})$, albeit only logarithmically.
However, in practice a small constant $\delta$ such as $0.01$ likely suffices.
With $\delta$ constant, we obtain a bound that is 
This bound is quite reasonable when $S \ll k(\mathcal{A})$, the number of nonzero entries in the tensor.
Because it is constant with respect to the number of nonzeroes, the bound on $S$ could (and does, for realistic settings of $\eps$ and $\delta$) exceed the number of nonzeros.
This is fundamental to bounds based on Hoeffding's inequality, and methods based on sampling-with-replacement in general.
In the next section, we obtain a bound on the number of samples needed that scales ``smoothly" with the number of nonzeroes, and critically never exceeds it.

\subsection{A concentration bound when sampling without replacement}

Recall that our algorithm can be viewed as sampling a set of random locations in the tensor, and then evaluating the \emph{deterministic} function~$x_g$ for various parameter settings $g$ at that location.
In the previous section, we imagined sampling the locations with replacement, so that the selected nonzeroes are independent.
By a stochastic domination argument, we could easily extend this bound to the case where the locations are sampled without replacement, but the bound remains weak in the regime where the minimal $S$ and $k(\mathcal{A})$ are similar in size.
Here, we use the following recent concentration bounds for sampling \emph{without replacement} due to Bardenet and Maillard \cite{BM2015}:

\begin{thm}[Hoeffding-Serfling inequality \cite{BM2015}]
 \label{thm:hoeffding-serfling}
 Let $\chi = \{x_1, x_2, \dots x_N\}$ be a finite population of $N > 1$ real points with $a = \min_i x_i$ and $b = \max_i x_i$.
Let $(X_1, X_2, \dots X_n)$ be a list of size $n < N$ sampled without replacement from $\chi$.
Then for all $\eps > 0$, we have
\[
\Pr\left[\frac{1}{n} \sum_{i = 1}^n X_i - \frac{1}{N} \sum_{i = 1}^N x_i \geq \eps \right]
\leq \exp \left( - \frac{2 n \eps^2}{(1 - n/N) (1 + 1/n) (b - a)^2} \right)
\]
\end{thm}

Here, our finite populations are $\chi_g$, the images of the nonzeroes under the functions $x_g$.
Observe that our algorithm as stated in \Cref{sec:algorithm} samples points independently, but without replacement, and so we can readily apply \Cref{thm:hoeffding-serfling}.

\begin{thm} \label{thm:without-replacement}
Let $D = \frac{N B^N}{2 \eps^2} \log \left(\frac{2B}{\delta}\right)$.
If $S > 0$ is at least the largest root of the quadratic equation
\begin{equation} \label{eq:quadratic}
\left( 1 - \frac{D}{k(\mathcal{A})} \right) n^2 + \left(D - \frac{D}{k(\mathcal{A})} \right) n + D = 0
\end{equation}
then
\[
\Pr\left[ \bigcup_g (1 - \eps) f_g \leq F_g \leq (1 + \eps) f_g \right]
\geq 1 - \delta
\]
\end{thm}

\begin{proof}
Suppose that $s_0$ satisfies \Cref{eq:quadratic}.
Then by rearranging the equation and substituting the definition of $D$ we have that
\[
\frac{N B^N}{2 \eps^2} \log \left(\frac{2B}{\delta}\right)
= \frac{s_0}{(1 - s_0/k(\mathcal{A}))(1 + 1/n)}
\]
Rearranging further and inverting the logarithm to isolate $\delta$, we obtain
\begin{equation} \label{eq:long-derivation}
\delta = 2 B^N \exp \left( - \frac{2s_0 \eps^2}{B^N (1 - s_0/k(\mathcal{A}) (1 + 1/s_0)} \right)
\end{equation}
Since $S \geq s_0$, we have that
\begin{equation} \label{eq:upper-bound}
\Pr[(1 - \eps) f_g \leq F_G \leq (1 + \eps) f_g]
\leq \Pr[(1 - \eps) f_g \leq F_G' \leq (1 + \eps) f_g]
\end{equation}
where $F_G' = \frac{1}{s_0} \sum_{i = 1}^{s_0} X_i$, which is an average of $s_0$ samples without replacement.
Notice that $\E[F_G'] = f_g$, and so by \Cref{thm:hoeffding-serfling},
\[
\Pr[F_G' \notin (1 \pm \eps)f_g]
\leq 2 \exp \left( - \frac{2s_0 \eps^2}{(1 - s_0/k(\mathcal{A}) (1 + 1/s_0)} \right)
\]
for any parameter setting~$g$.
Taking the union bound over the $B^N$ such parameteres, we obtain that the probability that we have more than $\eps$ relative error for \emph{any} parameter setting is
\[
\Pr[F_G' \notin (1 \pm \eps)f_g]
\leq 2 B^N exp \left( - \frac{2s_0 \eps^2}{(1 - s_0/k(\mathcal{A}) (1 + 1/s_0)} \right) = \delta
\]
by \Cref{eq:long-derivation}, which completes the proof in combination with \Cref{eq:upper-bound}.
\end{proof}


    \subsection{Runtime Analysis}
      \todo{Here, we will bound (very tightly) the number of operations per sample needed}